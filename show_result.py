"""
Script to display training results including MAE, MSE, and TECA metrics
"""
import torch
import pandas as pd
import json


def display_results(result_path):
    """
    Load and display training results from .pt file

    Args:
        result_path: Path to the .pt result file
    """
    print(f"Loading results from: {result_path}")
    print("=" * 80)

    # Load the results
    results = torch.load(result_path, map_location='cpu', weights_only=False)

    print("\nüìä Available keys in results:")
    for key in results.keys():
        print(f"  - {key}")

    # Display training history
    if 'loss_train_history' in results and 'loss_valid_history' in results:
        print("\n" + "=" * 80)
        print("üìà Training History:")
        print("=" * 80)
        train_loss = results['loss_train_history']
        valid_loss = results['loss_valid_history']

        for epoch, (t_loss, v_loss) in enumerate(zip(train_loss, valid_loss), 1):
            print(f"Epoch {epoch:3d}  |  Train Loss: {t_loss:.6f}  |  Valid Loss: {v_loss:.6f}")

    # Display test metrics
    if 'test_metrics_timestamp' in results:
        print("\n" + "=" * 80)
        print("üéØ Test Set Metrics (Timestamp-level):")
        print("=" * 80)

        test_metrics = results['test_metrics_timestamp']

        if isinstance(test_metrics, dict):
            for metric_name, value in test_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"{metric_name:30s}: {value:.6f}")
                else:
                    print(f"{metric_name:30s}: {value}")
        else:
            print(test_metrics)

    # Display test window metrics
    if 'test_metrics_win' in results:
        print("\n" + "=" * 80)
        print("üéØ Test Set Metrics (Window-level):")
        print("=" * 80)

        test_metrics = results['test_metrics_win']

        if isinstance(test_metrics, dict):
            for metric_name, value in test_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"{metric_name:30s}: {value:.6f}")
                else:
                    print(f"{metric_name:30s}: {value}")
        else:
            print(test_metrics)

    # Display validation metrics
    if 'valid_metrics_timestamp' in results:
        print("\n" + "=" * 80)
        print("‚úÖ Validation Set Metrics (Timestamp-level):")
        print("=" * 80)

        valid_metrics = results['valid_metrics_timestamp']

        if isinstance(valid_metrics, dict):
            for metric_name, value in valid_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"{metric_name:30s}: {value:.6f}")
                else:
                    print(f"{metric_name:30s}: {value}")
        else:
            print(valid_metrics)

    # Display validation window metrics
    if 'valid_metrics_win' in results:
        print("\n" + "=" * 80)
        print("‚úÖ Validation Set Metrics (Window-level):")
        print("=" * 80)

        valid_metrics = results['valid_metrics_win']

        if isinstance(valid_metrics, dict):
            for metric_name, value in valid_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"{metric_name:30s}: {value:.6f}")
                else:
                    print(f"{metric_name:30s}: {value}")
        else:
            print(valid_metrics)

    # region ËÑöÊú¨Ëß£Èáä
    # Display comparison between validation and test
    if 'valid_metrics_timestamp' in results and 'test_metrics_timestamp' in results:
        print("\n" + "=" * 80)
        print("üìä Validation vs Test Comparison (Timestamp-level):")
        print("=" * 80)
        print("üí° Tip: Test Set is the FINAL evaluation - this is what you should report!")
        print("-" * 80)

        valid_metrics = results['valid_metrics_timestamp']
        test_metrics = results['test_metrics_timestamp']

        # Define key metrics to compare
        key_metrics = ['MAE', 'RMSE', 'TECA', 'F1_SCORE', 'ACCURACY', 'PRECISION', 'RECALL']

        print(f"{'Metric':<20} {'Validation':>15} {'Test':>15} {'Difference':>15}")
        print("-" * 80)

        for metric in key_metrics:
            if metric in valid_metrics and metric in test_metrics:
                valid_val = valid_metrics[metric]
                test_val = test_metrics[metric]
                diff = test_val - valid_val

                # Add emoji indicator
                if metric in ['MAE', 'RMSE', 'MSE']:
                    # Lower is better
                    emoji = "‚úÖ" if diff < 0 else "‚ö†Ô∏è"
                else:
                    # Higher is better
                    emoji = "‚úÖ" if diff > 0 else "‚ö†Ô∏è"

                print(f"{metric:<20} {valid_val:>15.6f} {test_val:>15.6f} {diff:>14.6f} {emoji}")

        print("-" * 80)
        print("‚úÖ = Test performs better than Validation")
        print("‚ö†Ô∏è  = Test performs worse than Validation (may indicate overfitting)")

    # endregion

    # Display aggregated metrics (MAE, MSE, TECA at different time scales)
    for freq in ['D', 'W', 'ME']:
        key = f'test_metrics_{freq}'
        if key in results:
            print("\n" + "=" * 80)
            print(f"üìÖ Test Metrics Aggregated by {freq} (Day/Week/Month):")
            print("=" * 80)

            metrics = results[key]
            if isinstance(metrics, dict):
                for metric_name, value in metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"{metric_name:30s}: {value:.6f}")
                    else:
                        print(f"{metric_name:30s}: {value}")

    # Display predictions if available
    if 'test_outputs' in results:
        print("\n" + "=" * 80)
        print("üîÆ Prediction Statistics:")
        print("=" * 80)
        outputs = results['test_outputs']
        if isinstance(outputs, dict):
            if 'y_true' in outputs and 'y_pred' in outputs:
                y_true = outputs['y_true']
                y_pred = outputs['y_pred']
                print(f"Number of predictions: {len(y_pred)}")
                print(f"True values - Mean: {y_true.mean():.2f}, Std: {y_true.std():.2f}, Min: {y_true.min():.2f}, Max: {y_true.max():.2f}")
                print(f"Pred values - Mean: {y_pred.mean():.2f}, Std: {y_pred.std():.2f}, Min: {y_pred.min():.2f}, Max: {y_pred.max():.2f}")

    print("\n" + "=" * 80)
    print("‚ú® Results display completed!")
    print("=" * 80)

    return results


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        result_file = sys.argv[1]
    else:
        # Default path based on the training command
        # ‰øÆÊîπË¶ÅËØªÂèñÁöÑÊ®°Âûã
        result_file = "result/UKDALE_WashingMachine_1min/128/NILMFormer_0.pt"

    try:
        results = display_results(result_file)
    except FileNotFoundError:
        print(f"Error: Result file not found at {result_file}")
        print("\nUsage: python show_result.py [path_to_result.pt]")
        print(f"Example: python show_result.py result/UKDALE_WashingMachine_1min/128/NILMFormer_0.pt")
    except Exception as e:
        print(f"Error loading results: {e}")
        import traceback
        traceback.print_exc()
